{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRRXU8B8M7gf"
   },
   "source": [
    "Set up working environment\n",
    "- Github library for code\n",
    "- Working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icaqFq71M7gg",
    "outputId": "7c86dd95-1292-4d15-eb42-edc1e1df0acc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'ImageClassification' already exists and is not an empty directory.\n",
      "/content/ImageClassification\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jamespuckett23/ImageClassification.git\n",
    "\n",
    "import sys\n",
    "sys.path.append('/ImageClassification')\n",
    "\n",
    "%cd ImageClassification/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTaDBj1nM7gi"
   },
   "source": [
    "It may be necessary to update the library. If so, run the following block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQRuFRK3M7gi",
    "outputId": "9c59f4e9-5053-4687-a7e6-6b48ef7db3f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBZ2AN9J-VQx"
   },
   "source": [
    "Load functions from main.py to execute the DL model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Is6QK01b-uXy"
   },
   "outputs": [],
   "source": [
    "from ImageUtils import parse_record\n",
    "from DataReader import load_data, train_vaild_split\n",
    "from Model import Cifar\n",
    "\n",
    "import os\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xbhM-Zjm-gED",
    "outputId": "6af62fd4-a085-418a-808a-fd3ef8e7ce00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized arguments: ['-f', '/root/.local/share/jupyter/runtime/kernel-7fcc33a7-53b3-4a21-9cc3-1b02b45a31dd.json']\n",
      "--- Preparing Data ---\n",
      "### Training... ###\n",
      "Epoch 1 Loss 1.731594 Duration 25.237 seconds.\n",
      "Epoch 2 Loss 1.497890 Duration 24.597 seconds.\n",
      "Epoch 3 Loss 1.298513 Duration 24.767 seconds.\n",
      "Epoch 4 Loss 1.191681 Duration 24.689 seconds.\n",
      "Epoch 5 Loss 1.211403 Duration 24.834 seconds.\n",
      "Epoch 6 Loss 1.163448 Duration 24.565 seconds.\n",
      "Epoch 7 Loss 1.055747 Duration 24.614 seconds.\n",
      "Epoch 8 Loss 1.057674 Duration 24.684 seconds.\n",
      "Epoch 9 Loss 0.876021 Duration 24.637 seconds.\n",
      "Epoch 10 Loss 1.083756 Duration 24.616 seconds.\n",
      "Checkpoint has been created.\n",
      "Epoch 11 Loss 0.918391 Duration 24.593 seconds.\n",
      "Epoch 12 Loss 0.915031 Duration 24.485 seconds.\n",
      "Epoch 13 Loss 1.052115 Duration 24.540 seconds.\n",
      "Epoch 14 Loss 0.993616 Duration 24.425 seconds.\n",
      "Epoch 15 Loss 0.965676 Duration 24.372 seconds.\n",
      "Epoch 16 Loss 1.046457 Duration 24.407 seconds.\n",
      "Epoch 17 Loss 0.983264 Duration 24.247 seconds.\n",
      "Epoch 18 Loss 0.820832 Duration 24.477 seconds.\n",
      "Epoch 19 Loss 0.820624 Duration 24.619 seconds.\n",
      "Epoch 20 Loss 0.699962 Duration 24.621 seconds.\n",
      "Checkpoint has been created.\n",
      "Epoch 21 Loss 0.796956 Duration 24.505 seconds.\n",
      "Epoch 22 Loss 1.086537 Duration 24.432 seconds.\n",
      "Epoch 23 Loss 0.929844 Duration 24.526 seconds.\n",
      "Epoch 24 Loss 0.791047 Duration 24.367 seconds.\n",
      "Epoch 25 Loss 0.951252 Duration 24.260 seconds.\n",
      "Epoch 26 Loss 0.994140 Duration 24.813 seconds.\n",
      "Epoch 27 Loss 0.667179 Duration 24.442 seconds.\n",
      "Epoch 28 Loss 0.928908 Duration 24.320 seconds.\n",
      "Epoch 29 Loss 0.976645 Duration 24.410 seconds.\n",
      "Epoch 30 Loss 0.717861 Duration 24.376 seconds.\n",
      "Checkpoint has been created.\n",
      "Epoch 31 Loss 0.644646 Duration 24.371 seconds.\n",
      "Epoch 32 Loss 0.678645 Duration 24.209 seconds.\n",
      "Epoch 33 Loss 0.791217 Duration 24.258 seconds.\n",
      "Epoch 34 Loss 0.730552 Duration 24.199 seconds.\n",
      "Epoch 35 Loss 0.654844 Duration 24.355 seconds.\n",
      "Epoch 36 Loss 0.570445 Duration 24.418 seconds.\n",
      "Epoch 37 Loss 0.866867 Duration 24.293 seconds.\n",
      "Epoch 38 Loss 0.549790 Duration 24.305 seconds.\n",
      "Epoch 39 Loss 0.726562 Duration 24.249 seconds.\n",
      "Epoch 40 Loss 0.580762 Duration 24.145 seconds.\n",
      "Checkpoint has been created.\n",
      "Epoch 41 Loss 0.488882 Duration 24.332 seconds.\n",
      "Epoch 42 Loss 0.537372 Duration 24.309 seconds.\n",
      "Epoch 43 Loss 0.700635 Duration 24.142 seconds.\n",
      "Epoch 44 Loss 0.557494 Duration 24.104 seconds.\n",
      "Epoch 45 Loss 0.587025 Duration 24.188 seconds.\n",
      "Epoch 46 Loss 0.521868 Duration 24.025 seconds.\n",
      "Epoch 47 Loss 0.410637 Duration 24.176 seconds.\n",
      "Epoch 48 Loss 0.538941 Duration 24.113 seconds.\n",
      "Epoch 49 Loss 0.630450 Duration 24.107 seconds.\n",
      "Epoch 50 Loss 0.462580 Duration 24.177 seconds.\n",
      "Checkpoint has been created.\n",
      "Epoch 51 Loss 0.719151 Duration 24.044 seconds.\n",
      "Epoch 52 Loss 0.432071 Duration 24.200 seconds.\n",
      "Epoch 53 Loss 0.430382 Duration 24.143 seconds.\n",
      "Epoch 54 Loss 0.583870 Duration 24.281 seconds.\n",
      "Epoch 55 Loss 0.685595 Duration 24.136 seconds.\n",
      "Epoch 56 Loss 0.620590 Duration 24.095 seconds.\n",
      "Epoch 57 Loss 0.465057 Duration 24.162 seconds.\n",
      "Epoch 58 Loss 0.533708 Duration 24.145 seconds.\n",
      "Epoch 59 Loss 0.619849 Duration 24.053 seconds.\n",
      "Epoch 60 Loss 0.524446 Duration 24.199 seconds.\n",
      "Checkpoint has been created.\n",
      "Epoch 61 Loss 0.560993 Duration 24.151 seconds.\n",
      "Epoch 62 Loss 0.441753 Duration 24.243 seconds.\n",
      "Epoch 63 Loss 0.689408 Duration 24.219 seconds.\n",
      "Epoch 64 Loss 0.593424 Duration 24.296 seconds.\n",
      "Epoch 65 Loss 0.667940 Duration 23.996 seconds.\n",
      "Epoch 66 Loss 0.595304 Duration 24.109 seconds.\n",
      "Epoch 67 Loss 0.484984 Duration 24.285 seconds.\n",
      "Epoch 68 Loss 0.480446 Duration 24.147 seconds.\n",
      "Epoch 69 Loss 0.488302 Duration 24.144 seconds.\n",
      "Epoch 70 Loss 0.572422 Duration 24.545 seconds.\n",
      "Checkpoint has been created.\n",
      "Epoch 71 Loss 0.383124 Duration 24.430 seconds.\n",
      "Epoch 72 Loss 0.565523 Duration 24.242 seconds.\n",
      "Epoch 73 Loss 0.592808 Duration 24.353 seconds.\n",
      "Epoch 74 Loss 0.424019 Duration 24.176 seconds.\n",
      "Epoch 75 Loss 0.320517 Duration 24.143 seconds.\n",
      "Epoch 76 Loss 0.347457 Duration 24.354 seconds.\n",
      "Epoch 77 Loss 0.443015 Duration 24.103 seconds.\n",
      "Epoch 78 Loss 0.432856 Duration 24.091 seconds.\n",
      "Epoch 79 Loss 0.494225 Duration 24.267 seconds.\n",
      "Epoch 80 Loss 0.507699 Duration 24.457 seconds.\n",
      "Checkpoint has been created.\n",
      "Epoch 81 Loss 0.430338 Duration 24.160 seconds.\n",
      "Epoch 82 Loss 0.417077 Duration 24.170 seconds.\n",
      "Epoch 83 Loss 0.577300 Duration 24.229 seconds.\n",
      "Epoch 84 Loss 0.535992 Duration 24.115 seconds.\n",
      "Epoch 85 Loss 0.380100 Duration 24.333 seconds.\n",
      "Epoch 86 Loss 0.345749 Duration 24.396 seconds.\n",
      "Epoch 87 Loss 0.325739 Duration 24.510 seconds.\n",
      "Epoch 88 Loss 0.377266 Duration 24.608 seconds.\n",
      "Epoch 89 Loss 0.394047 Duration 24.361 seconds.\n",
      "Epoch 90 Loss 0.437352 Duration 24.534 seconds.\n",
      "Checkpoint has been created.\n",
      "Epoch 91 Loss 0.434636 Duration 24.218 seconds.\n",
      "Epoch 92 Loss 0.578212 Duration 23.674 seconds.\n",
      "Epoch 93 Loss 0.361410 Duration 23.893 seconds.\n",
      "Epoch 94 Loss 0.449164 Duration 23.741 seconds.\n",
      "Epoch 95 Loss 0.502844 Duration 23.771 seconds.\n",
      "Epoch 96 Loss 0.506990 Duration 23.572 seconds.\n",
      "Epoch 97 Loss 0.543851 Duration 23.491 seconds.\n",
      "Epoch 98 Loss 0.617757 Duration 23.263 seconds.\n",
      "Epoch 99 Loss 0.447433 Duration 23.407 seconds.\n",
      "Epoch 100 Loss 0.588343 Duration 23.811 seconds.\n",
      "Checkpoint has been created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/ImageClassification/Model.py:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_name, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss curve saved as training_loss_curve.png\n",
      "### Test or Validation ###\n",
      "Restored model parameters from model_v1/model-80.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:23<00:00, 208.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7930\n",
      "Restored model parameters from model_v1/model-90.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:23<00:00, 209.23it/s]\n",
      "/content/ImageClassification/Model.py:129: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7922\n",
      "Restored model parameters from model_v1/model-100.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 2786/5000 [00:13<00:10, 218.50it/s]"
     ]
    }
   ],
   "source": [
    "def configure():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    ### YOUR CODE HERE\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help='training batch size')\n",
    "    parser.add_argument(\"--num_classes\", type=int, default=10, help='number of classes')\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=10,\n",
    "                        help='save the checkpoint when epoch MOD save_interval == 0')\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=2e-4, help='weight decay rate')\n",
    "    parser.add_argument(\"--modeldir\", type=str, default='model_v1', help='model directory')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.003)\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0)\n",
    "    parser.add_argument(\"--lr_step\", type=int, default=10, help='step size for learning rate scheduler')\n",
    "    parser.add_argument(\"--lr_decay\", type=float, default=1/1.5, help='decay rate for learning rate scheduler')\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # return parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    print(\"Unrecognized arguments:\", unknown)\n",
    "    return args\n",
    "\n",
    "def main(config):\n",
    "    print(\"--- Preparing Data ---\")\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    data_dir = \"./cifar-10-batches-py\"\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    x_train, y_train, x_test, y_test = load_data(data_dir)\n",
    "    x_train_new, y_train_new, x_valid, y_valid = train_vaild_split(x_train, y_train)\n",
    "\n",
    "    model = Cifar(config).cuda()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    model.train(x_train_new, y_train_new, 100)\n",
    "    model.test_or_validate(x_valid, y_valid, [80, 90, 100])\n",
    "    model.test_or_validate(x_test, y_test, [100])\n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "config = configure()\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(config.gpu)\n",
    "main(config)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
